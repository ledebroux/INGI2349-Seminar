\documentclass[compsoc]{IEEEtran}
\usepackage[utf8]{inputenc}
\author{Olivier Tilmans ~~\and~~ Leonard Debroux}
\title{A Survey of Failure Handling in SDN}
\begin{document}
\maketitle
\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
Software Defined Network, Protection, Restoration, Failure Detection.
\end{IEEEkeywords}

\section{Introduction}

\section{Detecting failures}

\section{Handling the failures}
\subsection{By using path protection}

\subsection{By using restoration techniques}

\section{Conclusion}

\bibliographystyle{IEEEtran}
\nocite{*}
\bibliography{biblio}
\appendix
\section{Paper topics}
\subsection{Verifying forwarding plane connectivity}
This article presents a way to detect and locate a single arbitrary node or link failure.
In order to do so, the controller will install multiple sets of static routing rules in the switches.

The first set is used to detect whether there is a failure or not in the network.
The main idea is to create an Euler cycle in the network (if the network does not allow one, the trick is to duplicate all link and build a directed graph, which will force the existence of such a cycle). The controller will then attach itself to a node in the network and send a control message. This message will be then propagated around the cycle and will loop back to the controller if all links and node are operating properly.

The second set of forwarding rules is used to locate the failure in the network. The controller will send a second control message which should loop back to the control after reaching a given node. The controller will keep repeating this operation, allowing it to perform a binary search over the nodes until it locates the link or node that has failed.

\textbf{Limitations:}
\begin{itemize}
	\item \textbf{Scalability} This only works in case of a single link or node failure. (As the same node can be in different places fo the cycle thus breaking it multiple times). This is suitable to networks of arbitrary size. Detecting a failure requires up to 6 times the number of links, and takes up to log(\#number of links) control messages.
	\item \textbf{Time} This does not perform recovery, but detection. The detection itself might be 'slow' wrt. forwarding plane speed depending on the size of the network.
	\item \textbf{Overhead} Extremely small as very few control messages have to be sent when no failures are present.
	\item \textbf{Load} The load is insignificant as the controller has a passive role.
\end{itemize}

\textbf{Issue treated}
\begin{itemize}
	\item Node failure
	\item Link failure
\end{itemize}

\textbf{Topic}
\begin{itemize}
	\item Detection
\end{itemize}

\subsection{FatTire: Declarative Fault Tolerance for SDN}
This paper introduces a new language constructs that enable a programmer to specify flow invariant, even in case of failure. This construction is then compiled in order to generate backup forwarding table entries using OpenFlow fast failover mechanism and installed on the switches.
The paper describes the language syntax as well as its implementation.

\textbf{Limitations:}
\begin{itemize}
	\item \textbf{Scalability} This will require switches to have huge forwarding tables if it wants to account for every possible failure (if the programmer enabled FatTire for all possible flows). Scales linearly with the number of interfaces and the number of flow destinations. It can handle arbitrary number of failures
	\item \textbf{Time} Quite fast as the longest part of it is taken by the detection mechanism. The recovery itself is preinstalled in the flow tables.
	\item \textbf{Load} Not constant load on the controller.
\end{itemize}

\textbf{Issue treated}
\begin{itemize}
	\item Node failure
	\item Link failure
\end{itemize}

\textbf{Topic}
\begin{itemize}
	\item Protection
\end{itemize}

\subsection{Automatic failure recovery for SDN}
This paper advocates the need to separate the failure recovery mechanism on the controller from the forwarding and policy logic. It presents such a framework which is built for the POX controller, and provides a simple module that will handle failures and recover from them allowing developer to write failure-agnostic code.

This framework works in two phases:
\begin{enumerate}
	\item When there are no failures in the network, it records all events that happen on the controller.
	\item When a failure is detected, it performs the recovery in two phases:
		\begin{description}
			\item \textbf{The replay phase} It creates a clean copy of the original controller, minus the failed nodes, and replay all events it has recorded, which leads to a new coherent controller state accounting for the failure.
			\item \textbf{The reconfiguration phase} It pushes the new network state to the controller and the switches
		\end{description}
\end{enumerate}

\textbf{Limitations:}
\begin{itemize}
	\item \textbf{Scalability} The model used to account for failure is always simpler than the real network, so it has virtually no impact.
	\item \textbf{Time} This is a "slow" process, as the configuration change has to be pushed from the controller to the switches after being signaled to the controller.
	\item \textbf{Load} Only when a failure occurs
\end{itemize}

\textbf{Issue treated}
\begin{itemize}
	\item Node failure
	\item Link failure
\end{itemize}

\textbf{Topic}
\begin{itemize}
	\item Recovery
\end{itemize}

\subsection{Enabling fast failure recovery in OpenFlow Networks}
The topic of this article is to present a way of doing fast failure recovery in openflow networks.
The idea is to make sure that upon the acknowledgment of a failure by the controller, there is an efficient way to quickly inform the openflow switches.

To do so, the article presents an algorithm on how to recover from a failure fast enough and on the whole network.
For the method to work, there are several requierments from the controller. These are the following:
\begin{itemize}
	\item The controller must know about the failure
	\item The controller must remember the paths it established
	\item The controller must be able to compute a new path
	\item The controller knows all the current flows in the network
\end{itemize}
The algorithm states that upon a link failure, a new path is computed for every path that goes through this link.
Then, in each switch of the network that has an entry related to the old broken path, the entry is delete and new ones are pushed to establish the new path.

The results in the article are restoration times of about 12ms. In large networks, the goal is to propose restoration times that are below 50ms.

\textbf{Limitations:}
\begin{itemize}
	\item \textbf{Scalability} Does not scale well as the controller needs to remember all established paths.
	\item \textbf{Time} The recovery process is "fast" as only few flow are recomputed, but it still needs to wait for the new flows to be installed.
	\item \textbf{Load} Only when a failure occurs
\end{itemize}

\textbf{Issue treated}
\begin{itemize}
	\item Node failure
	\item Link failure
\end{itemize}

\textbf{Topic}
\begin{itemize}
	\item Restoration
\end{itemize}

\subsection{Scalable Fault Management for OpenFlow}
This paper present an alternative solution to LLDP to monitor the network health. To do so, the switches are extended to be able to send and to react to OAM (Operation, Administration, and Maintenance) packets.

The advantage taken from this solution is that the controller does not have to worry about monitoring the network for failures anymore. If it had to do it, to be able to react within a certain limit of time (50ms), it would mean send a high amount of LLDP packets per second, per link and per end-to-end tunnel. This causes serious scalability issues in openflow.

In this solution, if a failure occurs, it is detected without the need of the packet to get back to the sender. When a OAM packet is received on the egress side of a tunnel, the switch updates a state accordingly to the packet. For each tunnel monitored at the switch, there is a timer that is updated for each monitoring packet received. If that timer expires, the switch notifies the controller that the tunnel has failed.

\textbf{Limitations:}
\begin{itemize}
	\item \textbf{Scalability} The switches must be somehow intelligents, the advantage of having simple switches is lost.
	\item \textbf{Time} There is no need for response from the targeted switch, the only time constraint is the time that must elapse before a link is considered broken.
	\item \textbf{Overhead} No more than LLDP.
\end{itemize}

\textbf{Issue treated}
\begin{itemize}
	\item Node failure
	\item Link failure
\end{itemize}

\textbf{Topic}
\begin{itemize}
	\item Detection
\end{itemize}

\subsection{Network Architecture for joint Failure Recovery and Traffic Engineering}
This paper presents a way to unify load balancing and failure recovery.

It first starts by presenting this unified approach in a protocol agnostic way, then presents some experimental results and finally concludes by providing a few hints on how to deploy it in real networks (i.e. by using MPLS, Openflow, \ldots).

The main idea is that routers should be simple forwarding switches, much like in the SDN paradigm. An external management system precomputes multiple paths for each pair of edge routers which have traffic between them. The routers in the network will then perform load balancing by splitting the traffic over the normal paths and the backup paths. If a path fails, the routers will simply remove the failed path from their FIB, and continue forwarding traffic over the remaining paths.

In this system, failures are detected at a path level, by the ingress/egress routers, which will react to the failures themselves by rebalancing the traffic on the remaining path, ignoring the load informations. As this recovery is perform without this information, the edge routers do not need to broadcast real-time updates about the network state, which means that the reaction of a given router to a failure is deterministic. 

\textbf{Limitations:}
\begin{itemize}
	\item \textbf{Scalability} The switches must be able to remember several paths to each destination. 
	\item \textbf{Time} Only time consuming when the paths must be computed. Takes no time when a link goes down.
\end{itemize}

\textbf{Issue treated}
\begin{itemize}
	\item Node failure
	\item Link failure
\end{itemize}

\textbf{Topic}
\begin{itemize}
	\item Protection
	\item Recovery
\end{itemize}

\subsection{Ensuring connectivity via data plane mechanisms}
This article presents a way to speed up connectivity by assigning it to the data plane.

In the case of failures, the control plane is typicaly responsible for the recovery and is slower than the data plane by several orders of magnitude. What is proposed here is to delagate to the data plane the task of ensuring connectivity from point to point, so that, in the event of a failure, the recovery is done at the speed of the data plane.

To do so, they implement a variant of the Gafni-Bertsekas link reversal algorithm to build a directed acyclic graph for each destination, where each destination is a sink (no outgoing edges). That way, any path through that tree leads to the destination.

The control plane still keeps a role, that is to ensure that the paths that are used are optimal (eg: shortest paths). The algorithm used is incremental and aims at leading the networks towards shortest paths.

\textbf{Limitations:}
\begin{itemize}
	\item \textbf{Scalability} The aims of sdn is to provide a splitted way of using the data plane and control plane. Here, the bond between them is much stronger.
\end{itemize}

\textbf{Issue treated}
\begin{itemize}
	\item Node failure
	\item Link failure
\end{itemize}

\textbf{Topic}
\begin{itemize}
	\item Detection
	\item Protection
	\item Restoration
	\item Recovery
\end{itemize}
\end{document}